<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>XS3310 Teoría Estadística</title>
    <meta charset="utf-8" />
    <meta name="author" content="Prof. Shu Wei Chou Chen" />
    <script src="XS3310-I23_1_files/header-attrs/header-attrs.js"></script>
    <link href="XS3310-I23_1_files/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="text_color.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# XS3310 Teoría Estadística
]
.subtitle[
## II-2023
]
.author[
### Prof. Shu Wei Chou Chen
]
.institute[
### Escuela de Estadística
]
.date[
### 15-08-2023
]

---






---

### Información importante


1. [Mediación Virtual](https://mv1.mediacionvirtual.ucr.ac.cr/course/view.php?id=32768)
2. [Programa del curso](https://shuwei325.github.io/XS3310-II23/Programa-XS3310.pdf)
3. [Llenar el formulario](https://forms.gle/GwQ1YXYzgeL4ur1W6)


###Reglamentación

- [El Reglamento de Régimen Académico Estudiantil](https://www.cu.ucr.ac.cr/normativ/regimen_academico_estudiantil.pdf)
- [El Reglamento de Orden y Disciplina de los Estudiantes](https://www.cu.ucr.ac.cr/normativ/orden_y_disciplina.pdf)
-	[El Reglamento de la Universidad de Costa Rica contra el Hostigamiento Sexual](https://www.cu.ucr.ac.cr/normativ/hostigamiento_sexual.pdf)
  - Comisión Institucional Contra el Hostigamiento Sexual
    - https://hostigamientosexualucr.fcs.ucr.ac.cr/
    - https://www.ucr.ac.cr/comision-institucional-contra-el-hostigamiento-sexual.html





---

class: center, middle

## Hasta ahora en la carrera de Estadística:

 ¿Qué vimos en modelos probabilísticos discretos y contínuos? 

## https://seeing-theory.brown.edu



---

## ¿Para qué necesitamos Teoría Estadística?

Necesitamos entender el porqué los resultados teóricos funcionan. 

Es decir, si tomamos el promedio de un conjunto de dato, 

- ¿Es el mejor estimador que podemos generar? 
- ¿Qué significa mejor?
- ¿Puedo probar que efectivamente es el mejor?


---

## Repaso de Inferencia Estadística

- Variable aleatoria (v.a.)
- Muestra aleatoria (m.a.)
- Parámetro
- Estadístico
- Estimador 
- Modos de convergencia 
- Ley de lo grandes números 
- Teorema del límite central


---

## Parámetros, estadísticos y estimadores
	
### Variable aleatoria (v.a.)	

Una variable aleatoria (v.a.) `\({\displaystyle X}\)` es una función real definida en el espacio de probabilidad `\({\displaystyle (\Omega ,{\mathcal {A}},P)}\)`, asociado a un experimento aleatorio.

 `$${\displaystyle X:\Omega \to \mathbb {R} }$$`
---
### V.A. discreta

- Una variable aleatoria `\(Y\)` se dice que es discreta si puede tomar un número finito o contablemente infinito de valores distintos.

- La probabilidad de que `\(Y\)` tome el valor `\(y\)` es `\(P(Y=y)\)`.

- Generalmente se representa por medio de tabla o fórmula.

- Teorema:
  1. `\(0 \leq p(y) \leq 1\)` para toda `\(y\)`.
  2. `\(\sum\limits_{y} p(y)=1\)`.
  
---
### V.A. discreta
  
- Denote con `\(Y\)` cualquier variable aleatoria. La **función de distribución de `\(Y\)`**, denotada por `\(F(y)\)`, es tal que `\(F(y)=P(Y \leq y)\)` para `\(-\infty&lt;y&lt;\infty\)`.

- **Propiedades de una función de distribución:** Si `\(F(y)\)` es una función de distribución, entonces
  1. `\(F(-\infty) \equiv \lim\limits _{y \rightarrow-\infty} F(y)=0\)`.
  2. `\(F(\infty) \equiv \lim\limits _{y \rightarrow \infty} F(y)=1\)`.
  3. `\(F(y)\)` es una función no decreciente de `\(y\)`. [Si `\(y_1\)` y `\(y_2\)` son cualesquiera valores de manera que `\(y_1&lt;y_2\)`, entonces `\(F\left(y_1\right) \leq F\left(y_2\right)\)`.]

- **Valor esperado**

`$$\mu=E(Y)=\sum\limits_y y \cdot p(y)$$`
- **Variancia**

`$$Var(Y)=E\left[(Y-\mu)^2\right]=\sum\limits_y (y-\mu)^2$$`

---
### Ejemplo con la distribución binomial

Recuerden que la distribución binomial tiene la siguiente función de probabilidad:

`$$p(y)= \begin{cases}  \left(\begin{array}{l} n \\ y\end{array}\right) p^y q^{n-y}, \quad y=0,1,2, \ldots, n \text {  y  } 0 \leq p \leq 1 \\ 0 \quad \text{en otros casos} \end{cases}.$$`


1. Usando `\(n=3\)` y `\(p=0.4\)`, obtenga y grafique la función de probabilidad y la función de distribución.
2. Calcule su esperanza y variancia.


---

### V.A. continua

- Una variable aleatoria `\(Y\)` con función de de distribución `\(F(y)\)` se dice que es continua si `\(F(y)\)` es continua, para `\(-\infty&lt;y&lt;\infty\)`.

- Se puede deducir que `\(P(Y=y)=0\)`.

- Sea `\(F(y)\)` la función de distribución para una variable aleatoria continua `\(Y\)`. Entonces `\(f(y)\)`, dada por
`$$f(y)=\frac{d F(y)}{d y}=F^{\prime}(y)$$`
siempre que exista la derivada, se denomina función de densidad de probabilidad para
la variable aleatoria `\(Y\)`.

- Inversamente, se deduce `$$F(y)=\int_{-\infty}^y f(t) d t.$$`


---
### V.A. continua

- **Propiedades**: Si `\(f(y)\)` es una función de densidad para una variable aleatoria continua, entonces
  1. `\(f(y) \geq 0\)` para toda `\(y,-\infty&lt;y&lt;\infty\)`.
  2. `\(\int_{-\infty}^{\infty} f(y) d y=1\)`.

  
- Valor esperado

`$$\mu=E(Y)=\int\limits_{-\infty}^{\infty} y \cdot f(y) dy$$`

- Variancia

`$$\mu=E(Y)=\int\limits_{-\infty}^{\infty} y \cdot f(y) dy$$`

`$$Var(Y)=E\left[(Y-\mu)^2\right]=\int\limits_{-\infty}^{\infty} (y-\mu)^2  f(y) dy.$$`

---
### Ejemplo con la distribución beta


Suponga que `\(Y\)` tiene una distribución beta con parámetros `\(\alpha&gt;0\)` y `\(\beta&gt;0\)`, es decir
`$$f(y)= \begin{cases}\frac{y^{\alpha-1}(1-y)^{\beta-1}}{B(\alpha, \beta)}, &amp; 0 \leq y \leq 1 \\ 0, &amp; \text { otro caso}\end{cases}$$`

donde
$$
B(\alpha, \beta)=\int_0^1 y^{\alpha-1}(1-y)^{\beta-1} d y=\frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)} .
$$
1. Usando `\(\alpha=2\)` y `\(\beta=1\)`, obtenga y grafique la función de densidad y la función de distribución.
2. Obtenga su esperanza y variancia.

---

## Parámetros, estadísticos y estimadores

### Muestra aleatoria (m.a.)

Sean `\(X_{1}, X_{2}, ... , X_{n}\)` un conjunto de `\(n\)` variables aleatorias (v.a.) independientes e idénticamente distribuidas; este conjunto se denomina *muestra aleatoria* de una población infinita.


### Parámetro

Es una característica de la población. Algunos parámetros de interés podría ser la media, varianza o la proporción en una población.

### Estadístico

Es una función de la muestra aleatoria, `\(T=f\left(X_{1}, X_{2}, ... , X_{n}\right)\)`. Un estadístico es a su vez una variable aleatoria y como tal tiene su propia distribución, denominada distribución muestral, con sus parámetros correspondientes.

---

## Parámetros, estadísticos y estimadores

### Estimador

Cuando un estadístico, llámese `\(\hat{\theta}\)`, se utiliza para aproximar el valor de un parámetro `\(\theta\)`, entonces se acostumbra llamar a ese estadístico con el nombre de estimador.

&gt; **Notación:** `\(\theta\)` parámetro a estimar.

&gt; `\(\hat{\theta}\)` estimador de `\(\theta\)`.

### Ejemplo:

* `\(\bar{X}\)` es un estimador de `\(\mu\)`
* `\(S^2\)` es un estimador de `\(\sigma^2\)`
* `\(\hat{p}\)` es un estimador de `\(p\)`
* `\(\hat{\theta}_1=\frac{X_1+X_n}{2}\)` es un estimador de `\(\mu\)`
* `\(\hat{\theta}_2=\frac{X_{(1)}+X_{(n)}}{2}\)` es un estimador de `\(\mu\)`

---

## Modos de convergencia 

### Desigualdad de Markov  

Si `\(X\)` es una v.a. tal que  `\(\operatorname{Pr}(X \geq 0)=1\)`. Entonces para cada número real  `\(t&gt;0\)`, se tiene que 

`\begin{equation}
\operatorname{Pr}(X \geq t) \leq \frac{E(X)}{t}
\end{equation}`

### Desigualdad de Chebyshev 

Si  `\(X\)` es una v.a. que cumple que  `\(\operatorname{Var}(X)\)` existe. Entonces para cada número  `\(t&gt;0\)`

$$
\operatorname{Pr}(|X-E(X)| \geq t) \leq \frac{\operatorname{Var}(X)}{t^{2}}
$$


---

## Modos de convergencia 

### Propiedades de la media muestral 

Sea `\(X_{1}, \ldots, X_{n}\)` una v.a. con una  distribución con media  `\(\mu\)` y varianza `\(\sigma^{2}\)`. Sea  `\(\bar{X}_{n}\)` la media muestral. Entonces,  `\(E\left(\overline{X}_{n}\right)=\mu\)` y `\(\operatorname{Var}\left(\bar{X}_{n}\right)=\sigma^{2} / n.\)`

### Convergencia en probabilidad 

Decimos que  `\(Z_{1}, Z_{2}, \ldots\)` de v.a. converge a `\(b\)` en probabilidad si para cada número  `\(\varepsilon&gt;0\)`,

`$$\lim _{n \rightarrow \infty} \operatorname{Pr}\left(\left|Z_{n}-b\right|&lt;\varepsilon\right)=1$$`

Esta propiedad se denota como

$$
Z_{n} \stackrel{p}{\longrightarrow} b.
$$

---
## Parámetros, estadísticos y estimadores 
### Ley de los grandes números 

Suponga que  `\(X_{1}, \ldots, X_{n}\)` es una muestra de una distribución con media `\(\mu\)` y varianza finita. Sea  `\(\bar{X}_{n}\)` la media muestral. Entonces, 
`\begin{equation*}
\bar{X}_{n} \stackrel{p}{\longrightarrow} \mu
\end{equation*}`

¿Cómo se puede probar este resultado usando la desigualdad de Chebyshev?


---
## Parámetros, estadísticos y estimadores 
### Convergencia en distribución 

Suponga que se tienen una secuencia de funciones de distribución `\(F_1, \ldots, F_n\)` para la muestra `\(X_1,\ldots,X_n\)`. Se dice que `\(X_1,\ldots,X_n,\ldots\)`  converge en distribución a `\(X^*\)`, cuya función de distribución es `\(F^*\)`, si 


`\begin{equation*}
\lim _{n \rightarrow \infty} F_{n}(x)=F^{*}(x).
\end{equation*}`
Se denota también como 
`\begin{equation*}
X_n \stackrel{d}{\longrightarrow} X^*.
\end{equation*}`

---
## Parámetros, estadísticos y estimadores 

### Teorema del límite central

Si `\(X_1,\ldots, X_n\)` es una muestra aleatoria de tamaño `\(n\)`, y la distribución tiene media `\(\mu\)` y varianza `\(0&lt;\sigma^2&lt; \infty\)`. Entonces se cumple que 

`$$Z_{n} = \frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \stackrel{d}{\longrightarrow} N\left(0,1\right) \quad si\quad n \rightarrow \infty$$`
o lo que es equivalente 
`\(\overline{X} \xrightarrow{\text{d}} N\left(\mu, \frac{\sigma^2}{n}\right) \quad si\quad n \rightarrow \infty.\)`

---
## Parámetros, estadísticos y estimadores 
### Relaciones entre los modos de convergencia 

Se puede probar que 
`\begin{equation*}
\text{Convergencia en Probabilidad} \implies \text{Convergencia en distribución}
\end{equation*}`

Para un resumen más detallado, les dejo este video donde se explica mucho mejor

https://www.youtube.com/watch?v=bTMnnrw0v2Y

---

&lt;!-- ## Parámetros, estadísticos y estimadores --&gt;

&lt;!-- ### Ejemplo: --&gt;

&lt;!-- 1. Media muestral: `\(\bar{X} =\frac{\sum_{i=1}^n{ {X}_i } }{n}\)` --&gt;

&lt;!-- Se sabe que `\(\displaystyle E\left(\bar{X}\right)=\mu\)` y `\(\displaystyle Var\left(\bar{X}\right)=\frac{\sigma^2}{n}\)`, donde `\(\mu\)` y `\(\sigma^2\)` son la media y variancia poblacional.  --&gt;

&lt;!-- La distribución de `\(\bar{X}\)` va a depender de la distribución de la población. No obstante, con muestras grandes podemos hacer uso del Teorema del Límite Central, el cual dice: --&gt;

&lt;!-- `$$Z_{n} = \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \xrightarrow{\text{d}} N\left(0,1\right) \quad si\quad n \rightarrow \infty$$` --&gt;

&lt;!-- O lo que es equivalente a decir que `\(\bar{X} \xrightarrow{\text{d}} N\left(\mu, \frac{\sigma^2}{n}\right) \quad si\quad n \rightarrow \infty\)`. --&gt;

&lt;!-- --- --&gt;

## Parámetros, estadísticos y estimadores
	
### Varianza muestral

 `$$S^{2} = \frac{\sum{\left(X_{j}-\bar{X}\right)^2}}{n-1}$$`

Podemos demostrar que `\(E\left(S^2\right)=\sigma^2\)` y `\(Var\left(S^2\right)=\frac{2\sigma^4}{n-1}\)`.


En la siguiente sección, probaremos que si la población es `\(N\left(\mu,\sigma^2\right)\)`, entonces, `$$\displaystyle \frac{\left(n-1\right)S^2}{\sigma^2} \sim \chi^{2}_\left(n-1\right).$$` 
Además sabemos que el valor esperado de una `\(\chi^2\)` son sus grados de libertad y la varianza son dos veces los grados de libertad. Por lo tanto,

$$ E\left(\frac{\left(n-1\right)S^2}{\sigma^2}\right) = n-1  \\
    \Rightarrow \frac{\left(n-1\right)E\left(S^2\right)}{\sigma^2} = n-1 \\
	  \Rightarrow E\left(S^2\right) = \sigma^2. $$

---

## Parámetros, estadísticos y estimadores
	

Podemos hacer lo mismo para encontrar la varianza cuando la población es Normal:

$$  Var\left(\frac{\left(n-1\right)S^2}{\sigma^2}\right) = 2\left(n-1\right) $$ 

$$ \Rightarrow \frac{\left(n-1\right)^2}{\sigma^4}Var\left(S^2\right) = 2\left(n-1\right) \\
\Rightarrow Var\left(S^2\right) = \frac{2\sigma^4}{n-1}. $$

---
## Parámetros, estadísticos y estimadores

En realidad la distribución `\(\chi^2\)` es la distribución muestral de `\(S^2\)`. O sea que si se extraen todas las muestras posibles de una población normal y a cada muestra se le calcula su varianza, se obtendrá la distribución muestral de varianzas.

Para estimar la varianza poblacional o la desviación estándar, se necesita conocer el estadístico `\(X^2\)`. Si se elige una muestra de tamaño `\(n\)` de una población normal con varianza `\(\sigma^2\)`, el estadístico:

`$$X^2= \displaystyle \frac{(n-1)S^2}{\sigma^2}$$`

tiene una distribución muestral que es una distribución `\(\chi^2\)` con `\(gl= n-1\)` grados de libertad, es decir, `$$\displaystyle \frac{\left(n-1\right)S^2}{\sigma^2} \sim \chi^{2}_\left(n-1\right)$$`. 




---

class: center, middle

# ¡Gracias!

Slides creadas via R package [**xaringan**](https://github.com/yihui/xaringan).

El chakra viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.name/knitr), and [R Markdown](https://rmarkdown.rstudio.com).



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
